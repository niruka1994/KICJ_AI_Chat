import streamlit as st
import os

from langsmith import Client
from langchain_core.runnables import RunnableConfig
from langchain_core.tracers import LangChainTracer
from langchain_core.tracers.run_collector import RunCollectorCallbackHandler
from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv
from langchain_core.messages import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate
from streamlit_feedback import streamlit_feedback

load_dotenv()
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
GOOGLE_API_KEY = os.environ.get('GOOGLE_API_KEY')
ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY')
LANGCHAIN_PROJECT = os.environ.get('LANGCHAIN_PROJECT')

# Langsmith í´ë¼ì´ì–¸íŠ¸ ê°ì²´ìƒì„±
client = Client()
ls_tracer = LangChainTracer(project_name=LANGCHAIN_PROJECT, client=client)
run_collector = RunCollectorCallbackHandler()
cfg = RunnableConfig()
cfg["callbacks"] = [ls_tracer, run_collector]
cfg["configurable"] = {"session_id": "any"}

# ì„¸ì…˜ì— ì €ì¥ë  ê°ì²´ë“¤
if "messages" not in st.session_state:
    st.session_state["messages"] = []
if "store" not in st.session_state:
    st.session_state["store"] = dict()
if "last_run" not in st.session_state:
    st.session_state["last_run"] = None

# í˜ì´ì§€ íˆ´ë°” ì„¤ì •
st.set_page_config(page_title="KICJ AI Chat", page_icon=":smiling_face_with_3_hearts:")

# í—¤ë” ìŠ¤íƒ€ì¼ ì„¤ì •
st.markdown(
    """
    <style>
        .sub-header {
            font-size: 18px;
            font-weight: bold;
            color: #696363;
            text-align: center;
        }
        .warning-text {
            font-size: 18px;
            color: red;
            font-weight: bold;
            text-align: center;
        }
        .center-text {
            text-align: center;
            font-size: 50px;
            font-weight: bold;
        }
        .custom-divider-rainbow {
        margin: 2em 0;
        height: 2px;
        background: linear-gradient(to right, 
            #FF0000, #FFA500, #FFFF00, 
            #008000, #0000FF, #4B0082, #EE82EE); /* ë¬´ì§€ê°œìƒ‰ */
        }
        .custom-divider-gray {
        margin: 2em 0;
        height: 2px;
        background: #e8e7e6;
        }
    </style>
    """
    , unsafe_allow_html=True)

# í—¤ë” ì‚½ì…
st.markdown('<div class="center-text">ğŸ¥° KICJ AI Chat</div>', unsafe_allow_html=True)
st.markdown('<div class="custom-divider-rainbow"></div>', unsafe_allow_html=True)
st.markdown('<div class="sub-header">ì´ í”„ë¡œê·¸ë¨ì€ GPT, Gemini, Claude ëª¨ë¸ì„ ì„ íƒí•˜ì—¬ ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤.</div>',
            unsafe_allow_html=True)
st.markdown('<div class="warning-text">AIëŠ” í—ˆìœ„ì •ë³´ë‚˜ ì˜¤ë¥˜ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìœ¼ë‹ˆ, í•­ìƒ ì¶”ê°€ ê²€ì¦ì„ ì§„í–‰í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.</div>', unsafe_allow_html=True)
st.markdown('<div class="custom-divider-rainbow"></div>', unsafe_allow_html=True)

# ìœ ì €ì˜ ì„¤ì • ë¶€ë¶„
model_choice = st.selectbox(
    "AI Model (Optional)",
    ('GPT-3.5-Turbo', 'GPT-4o', 'Gemini-1.5-Pro', 'Gemini-1.5-Flash', 'Claude-3.5-Sonnet', 'Claude-3-Sonnet',
     'Claude-3-Opus', 'Cluade-3-Haiku'),
    help="GPT=ë²”ìš©ì„±â†‘, Gemini=ìµœì‹ ì„±â†‘, Claude=ë…í•´ë ¥â†‘"
)

# ì˜¨ë„ ìŠ¬ë¼ì´ë”
# temperature = st.slider("ì°½ì˜ë ¥ (Optional)", 0.0, 1.0, 0.7)

# ê¸°ë³¸ ë©”ì‹œì§€ ì„¤ì •
default_system_message = "ì§ˆë¬¸ì— ëŒ€í•˜ì—¬ ìì„¸í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”."

# Instruction ìˆ˜ì • í•„ë“œ
system_message = st.text_input("Instruction (Optional)", default_system_message,
                               help="AIì—ê²Œ ì„ë¬´ë¥¼ ë¶€ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (Ex. ë‹µë³€ì„ ì˜ì–´ë¡œë§Œ ëŒ€ë‹µí•´.)")

# í—¤ë” ì‚½ì…
st.markdown('<div class="custom-divider-gray"></div>', unsafe_allow_html=True)


# GPT ì‚¬ìš©ì‹œ streamingì„ ìœ„í•œ í´ë˜ìŠ¤
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)


# í™”ë©´ ì¶œë ¥ì„ ìœ„í•œ ëŒ€í™” ê¸°ë¡
def print_messages():
    if "messages" in st.session_state and len(st.session_state["messages"]) > 0:
        for chat_message in st.session_state["messages"]:
            st.chat_message(chat_message.role).write(chat_message.content)


# í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬í•  ëˆ„ì  ì±„íŒ… íˆìŠ¤í† ë¦¬
def get_session_history(session_ids: str) -> BaseChatMessageHistory:
    if session_ids not in st.session_state["store"]:
        st.session_state["store"][session_ids] = ChatMessageHistory()
    return st.session_state["store"][session_ids]


# í™”ë©´ì— ëŒ€í™”ì¶œë ¥
print_messages()

if user_input := st.chat_input("ì§ˆë¬¸ì„ ì‹œì‘í•˜ì„¸ìš”!"):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}")
    st.session_state["messages"].append(ChatMessage(role="user", content=user_input))
    # AI ë‹µë³€
    with st.chat_message("assistant"):
        with st.spinner("ìƒì„±ì¤‘.."):
            stream_handler = StreamHandler(st.empty())
            if model_choice == 'GPT-3.5-Turbo':
                model_choice = 'gpt-3.5-turbo'
            elif model_choice == 'GPT-4o':
                model_choice = 'gpt-4o'
            elif model_choice == 'Gemini-1.5-Pro':
                model_choice = 'gemini-1.5-pro'
            elif model_choice == 'Gemini-1.5-Flash':
                model_choice = 'gemini-1.5-flash'
            elif model_choice == 'Claude-3.5-Sonnet':
                model_choice = 'claude-3-5-sonnet-20240620'
            elif model_choice == 'Claude-3-Sonnet':
                model_choice = 'claude-3-sonnet-20240229'
            elif model_choice == 'Claude-3-Opus':
                model_choice = 'claude-3-opus-20240229'
            elif model_choice == 'Cluade-3-Haiku':
                model_choice = 'claude-3-haiku-20240307'

            if model_choice.startswith('gemini'):
                llm = ChatGoogleGenerativeAI(model=model_choice,
                                             # temperature=temperature,
                                             )
            elif model_choice.startswith('gpt'):
                llm = ChatOpenAI(model=model_choice,
                                 # temperature=temperature,
                                 streaming=True,
                                 callbacks=[stream_handler])
            elif model_choice.startswith('claude'):
                llm = ChatAnthropic(model=model_choice,
                                    # temperature=temperature,
                                    streaming=True,
                                    callbacks=[stream_handler])

            prompt = ChatPromptTemplate.from_messages(
                [
                    ("system", system_message),
                    MessagesPlaceholder(variable_name="history"),
                    ("human", "{question}"),
                ]
            )
            chain = prompt | llm

            chain_with_memory = RunnableWithMessageHistory(
                chain,
                get_session_history,
                input_messages_key="question",
                history_messages_key="history"
            )

            try:
                response = chain_with_memory.invoke(
                    {"question": user_input}, cfg
                )
                st.session_state["messages"].append(ChatMessage(role="assistant", content=response.content))
                st.session_state.last_run = run_collector.traced_runs[0].id
                st.rerun()

            except Exception as e:
                st.write(f"ì—ëŸ¬ë°œìƒ: {e}")

if st.session_state.get("last_run"):
    feedback = streamlit_feedback(
        feedback_type="thumbs",
        optional_text_label="[ì„ íƒ] ì˜ê²¬ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.",
        key=f"feedback_{st.session_state.last_run}",
    )

    if feedback:
        scores = {"ğŸ‘": 1, "ğŸ‘": 0}
        client.create_feedback(
            st.session_state.last_run,
            feedback["type"],
            score=scores[feedback["score"]],
            comment=feedback.get("text", None),
        )
        st.toast("í”¼ë“œë°±ì´ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤.", icon="ğŸ¥°")